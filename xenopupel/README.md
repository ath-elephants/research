# 06.09
- убрал лик данных, когда аугм была перед сплитом
- сконкатил текст категорий и вопросов (дало импрув метрик)
- перевел все в нижний регистр (тоже дало прирост метрик)
- потестил бустинги, с нуля работают гораздо хуже линейки, у меня не хватает мощностей чтобы подобрать для них параметры
- попробовал замапить сокращения из глоссария (еще попробовал добавлять расшифровку следущим словом в скобках) - метрики чучуть падают

### Метрики кроссвалидации
#### Train Metrics:
**accuracy**: 0.9695 \
**precision**: macro: 0.9755; micro: 0.9695 \
**recall**: macro: 0.9710; micro: 0.9695 \
**f1**: macro: 0.9725; micro: 0.9695 

#### Test Metrics (чистый тест без аугм.):
**accuracy**: 0.8394 \
**precision**: macro: 0.7105, micro: 0.8394 \
**recall**: macro: 0.7323; micro: 0.8394 \
**f1**: macro: 0.7045; micro: 0.8394 


# 05.09 Поиск пересечений в ответ-категория, аугментаиция, получение эмбеддингов через Navec, предсказания через LR
В общем я тут делал по 30 аугментированных копий каждому ответу (самым частовстречающимся доп. копии не делал). Сделал эмбеддинги и обучил лин. регрессию, ничего не тюнил. Получились вот такие метрики на тестовой выборке: 

Accuracy: 0.8325 \
Precision: (macro:0.9057, micro:0.8325) \
Recall: (macro:0.8568, micro:0.8325) \
F1 Score: (macro:0.8639, micro:0.8325)  

### Я думал, что эти метрики только за счет супер редких наблюдений, которые получили копии и просто выучились, и это отчасти правда, но и частые классы тоже как-то угадываются:
![image](https://github.com/user-attachments/assets/60727de9-1d0d-4107-a415-d63cabdfa5a3)
