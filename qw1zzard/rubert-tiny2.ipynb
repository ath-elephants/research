{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\research\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "MODEL_NAME = 'cointegrated/rubert-tiny2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: list[str],\n",
    "        labels: list[str],\n",
    "        tokenizer: AutoTokenizer,\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            max_length=self.max_length, \n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes: int,\n",
    "        steps_per_epoch=None,\n",
    "        n_epochs=None,\n",
    "        lr=2e-5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(MODEL_NAME).train()\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.pre_classifier = nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, n_classes, bias=True),\n",
    "        )\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        self.train_accuracy = torchmetrics.Accuracy(\n",
    "            task='multiclass',\n",
    "            num_classes=n_classes,\n",
    "            average='macro',\n",
    "        )\n",
    "        self.val_accuracy = torchmetrics.Accuracy(\n",
    "            task='multiclass',\n",
    "            num_classes=n_classes,\n",
    "            average='macro',\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]\n",
    "        return self.classifier(outputs)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        self.train_accuracy.update(outputs.softmax(dim=-1), labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        accuracy = self.train_accuracy.compute()\n",
    "        self.log('train_accuracy', accuracy, on_epoch=True)\n",
    "        print(f'\\nEpoch {self.current_epoch} Train Accuracy: {accuracy:.4f}')\n",
    "        self.train_accuracy.reset()\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        self.val_accuracy.update(outputs.softmax(dim=-1), labels)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        accuracy = self.val_accuracy.compute()\n",
    "        self.log('val_accuracy', accuracy, on_epoch=True)\n",
    "        print(f'\\nEpoch {self.current_epoch} Validation Accuracy: {accuracy:.4f}')\n",
    "        self.val_accuracy.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "        texts: list[str],\n",
    "        labels: list[str],\n",
    "        max_length: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        texts, labels, test_size=0.1, random_state=42,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    train_dataset = TextDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "    val_dataset = TextDataset(val_texts, val_labels, tokenizer, max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        texts,\n",
    "        labels,\n",
    "        num_classes,\n",
    "        max_length=512,\n",
    "        batch_size=16,\n",
    "        n_epochs=5,\n",
    "        lr=2e-5,\n",
    "    ):\n",
    "    train_loader, val_loader, label_encoder = preprocess_data(texts, labels, max_length, batch_size)\n",
    "\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    model = BERTClassifier(\n",
    "        n_classes=num_classes,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        n_epochs=n_epochs,\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=n_epochs, \n",
    "        devices=1 if torch.cuda.is_available() else None,\n",
    "        accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    return model, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\research\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "d:\\GitHub\\research\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | bert           | BertModel          | 29.2 M | train\n",
      "1 | pre_classifier | Linear             | 97.7 K | train\n",
      "2 | classifier     | Sequential         | 76.1 K | train\n",
      "3 | loss_fn        | CrossEntropyLoss   | 0      | train\n",
      "4 | train_accuracy | MulticlassAccuracy | 0      | train\n",
      "5 | val_accuracy   | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------------\n",
      "29.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "29.4 M    Total params\n",
      "117.470   Total estimated model params size (MB)\n",
      "74        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\research\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\research\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  9.78it/s]\n",
      "Epoch 0 Validation Accuracy: 0.0000\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\research\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 95/95 [00:10<00:00,  9.31it/s, v_num=40]\n",
      "Epoch 0 Validation Accuracy: 0.0285\n",
      "Epoch 0: 100%|██████████| 95/95 [00:10<00:00,  8.95it/s, v_num=40]\n",
      "Epoch 0 Train Accuracy: 0.0047\n",
      "Epoch 1: 100%|██████████| 95/95 [00:10<00:00,  9.45it/s, v_num=40]\n",
      "Epoch 1 Validation Accuracy: 0.0298\n",
      "Epoch 1: 100%|██████████| 95/95 [00:10<00:00,  9.08it/s, v_num=40]\n",
      "Epoch 1 Train Accuracy: 0.0077\n",
      "Epoch 2: 100%|██████████| 95/95 [00:10<00:00,  9.47it/s, v_num=40]\n",
      "Epoch 2 Validation Accuracy: 0.0323\n",
      "Epoch 2: 100%|██████████| 95/95 [00:10<00:00,  9.10it/s, v_num=40]\n",
      "Epoch 2 Train Accuracy: 0.0080\n",
      "Epoch 3: 100%|██████████| 95/95 [00:09<00:00,  9.54it/s, v_num=40]\n",
      "Epoch 3 Validation Accuracy: 0.0379\n",
      "Epoch 3: 100%|██████████| 95/95 [00:10<00:00,  9.16it/s, v_num=40]\n",
      "Epoch 3 Train Accuracy: 0.0088\n",
      "Epoch 4: 100%|██████████| 95/95 [00:09<00:00,  9.56it/s, v_num=40]\n",
      "Epoch 4 Validation Accuracy: 0.0413\n",
      "Epoch 4: 100%|██████████| 95/95 [00:10<00:00,  9.18it/s, v_num=40]\n",
      "Epoch 4 Train Accuracy: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 95/95 [00:11<00:00,  8.41it/s, v_num=40]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Вопрос ответ.csv')\n",
    "\n",
    "texts = df['question'].to_list()\n",
    "labels = (df['category'] + '///' + df['content']).to_list()\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "model, label_encoder = train_model(texts, labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: поддержка///Создайте, пожалуйста, обращение в ИТ поддержку на портале support\n"
     ]
    }
   ],
   "source": [
    "def predict_class(model, tokenizer, text, label_encoder, device='cpu'):\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=64,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "    predicted_class_id = predictions.item()\n",
    "    predicted_class_text = label_encoder.inverse_transform([predicted_class_id])[0]\n",
    "\n",
    "    return predicted_class_text\n",
    "\n",
    "\n",
    "text = 'Как трудоустроится совместителем?'\n",
    "model.eval()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "predicted_class_text = predict_class(model, tokenizer, text, label_encoder, device)\n",
    "print(f'Predicted class: {predicted_class_text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
